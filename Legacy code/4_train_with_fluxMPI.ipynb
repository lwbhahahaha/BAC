{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.add(\"Optimisers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/Project BAC/BAC project/libs`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MPI Implementation is not CUDA Aware.\n",
      "└ @ FluxMPI /home/molloi-lab/.julia/packages/FluxMPI/OM5f6/src/FluxMPI.jl:28\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"libs/\")\n",
    "using Lux, Random, NNlib, Zygote, LuxCUDA, CUDA, FluxMPI, JLD2, DICOM\n",
    "using Images\n",
    "using MLUtils\n",
    "using Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 256\n",
    "patch_size_half = round(Int, patch_size/2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_num_of_imgs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This function zoom all pixel values into [0, 1].\n",
    "\"\"\"\n",
    "function zoom_pxiel_values(img)\n",
    "    a, b = minimum(img), maximum(img)\n",
    "    if b-a != 0\n",
    "        img = (img .- a) / (b - a)\n",
    "    end\n",
    "    return img\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    This function takes in a img of various size, \n",
    "    returns patches with size = patch_size * patch_size.\n",
    "\"\"\"\n",
    "function patch_image(img, lbl)\n",
    "    s = size(img)\n",
    "    x = ceil(Int, s[1]/patch_size) + floor(Int, (s[1]-patch_size_half)/patch_size)\n",
    "    y = ceil(Int, s[2]/patch_size) + floor(Int, (s[2]-patch_size_half)/patch_size)\n",
    "    num_patches = x*y\n",
    "    img_patches = Array{Float32, 4}(undef, patch_size, patch_size, 1, num_patches)\n",
    "    lbl_patches = Array{Float32, 4}(undef, patch_size, patch_size, 1, num_patches)\n",
    "    ct = 0\n",
    "    for i = 1 : x-1\n",
    "        x_start = 1+(i-1)*patch_size_half\n",
    "        x_end = x_start+patch_size-1\n",
    "        for j = 1 : y-1\n",
    "            y_start = 1+(j-1)*patch_size_half\n",
    "            y_end = y_start+patch_size-1\n",
    "            # save patch\n",
    "            ct += 1\n",
    "            img_patches[:, :, 1, ct] = zoom_pxiel_values(img[x_start:x_end, y_start:y_end])\n",
    "            lbl_patches[:, :, 1, ct] = lbl[x_start:x_end, y_start:y_end]\n",
    "        end\n",
    "        # right col\n",
    "        y_start, y_end = s[2]-patch_size+1, s[2]\n",
    "        # save patch\n",
    "        ct += 1\n",
    "        img_patches[:, :, 1, ct] = zoom_pxiel_values(img[x_start:x_end, y_start:y_end])\n",
    "        lbl_patches[:, :, 1, ct] = lbl[x_start:x_end, y_start:y_end]\n",
    "    end\n",
    "    # last row\n",
    "    x_start, x_end = s[1]-patch_size+1, s[1]\n",
    "    for j = 1 : y-1\n",
    "        y_start = 1+(j-1)*patch_size_half\n",
    "        y_end = y_start+patch_size-1\n",
    "        # save patch\n",
    "        ct += 1\n",
    "        img_patches[:, :, 1, ct] = zoom_pxiel_values(img[x_start:x_end, y_start:y_end])\n",
    "        lbl_patches[:, :, 1, ct] = lbl[x_start:x_end, y_start:y_end]\n",
    "    end\n",
    "    # right col\n",
    "    y_start, y_end = s[2]-patch_size+1, s[2]\n",
    "    # save patch\n",
    "    ct += 1\n",
    "    img_patches[:, :, 1, ct] = zoom_pxiel_values(img[x_start:x_end, y_start:y_end])\n",
    "    lbl_patches[:, :, 1, ct] = lbl[x_start:x_end, y_start:y_end]\n",
    "    # return\n",
    "    return num_patches, img_patches, lbl_patches\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    This function fixs the path to the images and labels.\n",
    "\"\"\"\n",
    "function fix_path!(data_set)\n",
    "    num_data = size(data_set)[1]\n",
    "    Threads.@threads for i = 1 : num_data\n",
    "        for j = 1 : 2\n",
    "            for k = 1 : 4\n",
    "                # modify img path\n",
    "                splited = split(deepcopy(data_set[i][j][k]), \"\\\\\")\n",
    "                if size(splited)[1] > 1\n",
    "                    new_path = joinpath(\"../collected_dataset_for_ML\", joinpath(splited[4:end]))\n",
    "                    data_set[i][j][k] = new_path\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    This function check how many number of images and labels there will be after patching.\n",
    "\"\"\"\n",
    "function get_num_of_imgs(data_set)\n",
    "    num_data = size(data_set)[1]\n",
    "    cts = Array{Int}(undef, num_data*4)\n",
    "    Threads.@threads for i = 1 : num_data\n",
    "        @views t = train_set[i]\n",
    "        for j = 1 : 4\n",
    "            # read dicom images\n",
    "            s = size(dcm_parse(t[1][j])[(0x7fe0, 0x0010)])\n",
    "            x = ceil(Int, s[1]/patch_size) + floor(Int, (s[1]-patch_size_half)/patch_size)\n",
    "            y = ceil(Int, s[2]/patch_size) + floor(Int, (s[2]-patch_size_half)/patch_size)\n",
    "            # save \n",
    "            cts[(i-1)*4+j] = x*y\n",
    "        end\n",
    "    end\n",
    "    return cts\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load \"clean_set_step2_for_ubuntu.jld2\" train_set valid_set\n",
    "\n",
    "data_dir = \"../collected_dataset_for_ML\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Load train set & valid set\n",
    "container format: patch_size * patch_size * 1 * num_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703276"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get num of total patches(train)\n",
    "ct_patches_train = get_num_of_imgs(train_set)\n",
    "num_patches_train = sum(ct_patches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime: 50s\n",
    "num_train_data = size(train_set)[1]\n",
    "train_container_images = Array{Float16, 4}(undef, patch_size, patch_size, 1, num_patches_train)\n",
    "train_container_masks = Array{Float16, 4}(undef, patch_size, patch_size, 1, num_patches_train)\n",
    "Threads.@threads for i = 1 : num_train_data\n",
    "    start_idx = sum(ct_patches_train[1:i-1])+1\n",
    "    for j = 1 : 4 # 4 images each patient\n",
    "        # read dicom images\n",
    "        img = Float16.(dcm_parse(train_set[i][1][j])[(0x7fe0, 0x0010)])\n",
    "        # read png images\n",
    "        lbl = Float16.(Images.load(train_set[i][2][j]))\n",
    "        # process image\n",
    "        num_patches, img_patches, lbl_patches = patch_image(img, lbl)\n",
    "        # save \n",
    "        end_idx = start_idx+num_patches-1\n",
    "        train_container_images[:, :, 1, start_idx : end_idx] = img_patches\n",
    "        train_container_masks[:, :, 1, start_idx : end_idx] = lbl_patches\n",
    "        start_idx = end_idx\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get num of total patches(valid)\n",
    "# ct_patches_valid = get_num_of_imgs(valid_set)\n",
    "# num_patches_valid = sum(ct_patches_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # runtime: 7.5s\n",
    "# num_valid_data = size(valid_set)[1]\n",
    "# valid_container_images = Array{Float16, 4}(undef, patch_size, patch_size, 1, num_patches_valid)\n",
    "# valid_container_masks = Array{Float16, 4}(undef, patch_size, patch_size, 1, num_patches_valid)\n",
    "# Threads.@threads for i = 1 : num_valid_data\n",
    "#     start_idx = sum(ct_patches_valid[1:i-1])+1\n",
    "#     for j = 1 : 4 # 4 images each patient\n",
    "#         # read dicom images\n",
    "#         img = Float16.(dcm_parse(valid_set[i][1][j])[(0x7fe0, 0x0010)])\n",
    "#         # read png images\n",
    "#         lbl = Float16.(Images.load(valid_set[i][2][j]))\n",
    "#         # process image\n",
    "#         num_patches, img_patches, lbl_patches = patch_image(img, lbl)\n",
    "#         # save \n",
    "#         end_idx = start_idx+num_patches-1\n",
    "#         valid_container_images[:, :, 1, start_idx : end_idx] = img_patches\n",
    "#         valid_container_masks[:, :, 1, start_idx : end_idx] = lbl_patches\n",
    "#         start_idx = end_idx\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s = 20\n",
    "# test_loader = MLUtils.DataLoader((data=valid_container_images, label=valid_container_masks), batchsize=b_s)\n",
    "train_loader = MLUtils.DataLoader((data=train_container_images, label=train_container_masks), batchsize=b_s);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unet2D (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_conv = (in, out) -> Conv((3, 3), in=>out, pad=SamePad())\n",
    "\n",
    "conv1 = (in, out) -> Chain(_conv(in, out), BatchNorm(out, leakyrelu))\n",
    "conv2 = (in, out) -> Chain(_conv(in, out), x -> softmax(x; dims = 3))\n",
    "\n",
    "_tran = (in, out) -> ConvTranspose((2, 2), in => out, stride = 2)\n",
    "tran = (in, out) -> Chain(_tran(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "my_cat = (x, y) -> cat(x, y; dims=Val(3))\n",
    "\n",
    "function unet2D(in_chs, lbl_chs)    \n",
    "    # Contracting layers\n",
    "    l1 = Chain(conv1(in_chs, 64), conv1(64, 64))\n",
    "    l2 = Chain(l1, MaxPool((2,2), stride=2), conv1(64, 128), conv1(128, 128))\n",
    "    l3 = Chain(l2, MaxPool((2,2), stride=2), conv1(128, 256), conv1(256, 256))\n",
    "    l4 = Chain(l3, MaxPool((2,2), stride=2), conv1(256, 512), conv1(512, 512))\n",
    "    l5 = Chain(l4, MaxPool((2,2), stride=2), conv1(512, 1024), conv1(1024, 1024), tran(1024, 512))\n",
    "    \n",
    "    # Expanding layers\n",
    "    l6 = Chain(Parallel(my_cat,l5,l4), conv1(512+512, 512), conv1(512, 512), tran(512, 256))\n",
    "    l7 = Chain(Parallel(my_cat,l6,l3), conv1(256+256, 256), conv1(256, 256), tran(256, 128))\n",
    "    l8 = Chain(Parallel(my_cat,l7,l2), conv1(128+128, 128), conv1(128, 128), tran(128, 64))\n",
    "    l9 = Chain(Parallel(my_cat,l8,l1), conv1(64+64, 64), conv1(64, 64), conv2(64, lbl_chs))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Create Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dice_loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dice_loss(ŷ, y; ϵ=1f-5)\n",
    "    @inbounds loss_dice = \n",
    "        1f0 - (muladd(2f0, sum(ŷ[:,:,2,:] .* y[:,:,1,:]), ϵ) / (sum(ŷ[:,:,2,:] .^ 2) + sum(y[:,:,1,:] .^ 2) + ϵ))\n",
    "    return loss_dice\n",
    "end\n",
    "lossfn = dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FluxMPI.Init()\n",
    "CUDA.allowscalar(false)\n",
    "# Seeding\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Using `gpu` inside performance critical code will cause massive slowdowns due to type inference failure. Please update your code to use `gpu_device` API.\n",
      "└ @ Lux /home/molloi-lab/.julia/packages/Lux/5YzHA/src/deprecated.jl:28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_8 = NamedTuple(), layer_9 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_10 = NamedTuple(), layer_11 = NamedTuple(), layer_12 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_13 = NamedTuple(), layer_14 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_15 = NamedTuple(), layer_16 = NamedTuple(), layer_17 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_18 = NamedTuple(), layer_19 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_20 = NamedTuple(), layer_21 = NamedTuple(), layer_22 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_23 = NamedTuple(), layer_24 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_25 = NamedTuple(), layer_26 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_2 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_8 = NamedTuple(), layer_9 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_10 = NamedTuple(), layer_11 = NamedTuple(), layer_12 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_13 = NamedTuple(), layer_14 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_15 = NamedTuple(), layer_16 = NamedTuple(), layer_17 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_18 = NamedTuple(), layer_19 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()))), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_2 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_8 = NamedTuple(), layer_9 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_10 = NamedTuple(), layer_11 = NamedTuple(), layer_12 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_13 = NamedTuple(), layer_14 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()))), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_2 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_8 = NamedTuple(), layer_9 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()))), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_2 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()))), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = NamedTuple())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# device = gpu_device()\n",
    "model = unet2D(1, 2)\n",
    "\n",
    "ps, st = Lux.setup(rng, model) .|> gpu\n",
    "ps = FluxMPI.synchronize!(ps; root_rank = 0)\n",
    "st = FluxMPI.synchronize!(st; root_rank = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (layer_1 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_2 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_3 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_4 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_5 = NamedTuple(), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_8 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_9 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_10 = NamedTuple(), layer_11 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_12 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_13 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_14 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_15 = NamedTuple(), layer_16 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_17 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_18 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_19 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_20 = NamedTuple(), layer_21 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_22 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_23 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_24 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_25 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_26 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m)), layer_2 = (layer_1 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_2 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_3 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_4 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_5 = NamedTuple(), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_8 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_9 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_10 = NamedTuple(), layer_11 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_12 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_13 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_14 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_15 = NamedTuple(), layer_16 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_17 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_18 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_19 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m))), layer_2 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_3 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_4 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_5 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m)), layer_2 = (layer_1 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_2 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_3 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_4 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_5 = NamedTuple(), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_8 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_9 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_10 = NamedTuple(), layer_11 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_12 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_13 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_14 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m))), layer_2 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_3 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_4 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_5 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m)), layer_2 = (layer_1 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_2 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_3 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_4 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_5 = NamedTuple(), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_8 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_9 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m))), layer_2 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_3 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_4 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_5 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m)), layer_2 = (layer_1 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_2 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_3 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_4 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m))), layer_2 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_3 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_4 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; … ;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;; … ;;; 2.22045f-16;;; 2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_5 = (scale = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39mFloat32[2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16  …  2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16, 2.22045f-16]\u001b[32m)\u001b[39m), layer_6 = (weight = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; … ;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16;;; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16; 2.22045f-16 2.22045f-16 2.22045f-16]\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(DistributedOptimizer{AdaGrad{Float64}}(AdaGrad{Float64}(0.01, 2.22045e-16)), \u001b[39m[2.22045f-16;;; 2.22045f-16;;;;]\u001b[32m)\u001b[39m), layer_7 = NamedTuple())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = DistributedOptimizer(AdaGrad(0.01))\n",
    "st_opt = Optimisers.setup(opt, ps)\n",
    "\n",
    "st_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfGPUMemoryError",
     "evalue": "Out of GPU memory trying to allocate 160.000 MiB\nEffective GPU memory usage: 99.78% (22.078 GiB/22.126 GiB)\nMemory pool usage: 19.750 GiB (20.156 GiB reserved)\n",
     "output_type": "error",
     "traceback": [
      "Out of GPU memory trying to allocate 160.000 MiB\n",
      "Effective GPU memory usage: 99.78% (22.078 GiB/22.126 GiB)\n",
      "Memory pool usage: 19.750 GiB (20.156 GiB reserved)\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/tVtYo/src/pool.jl:435 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ ./timing.jl:393 [inlined]\n",
      "  [3] #_alloc#991\n",
      "    @ ~/.julia/packages/CUDA/tVtYo/src/pool.jl:424 [inlined]\n",
      "  [4] _alloc\n",
      "    @ ~/.julia/packages/CUDA/tVtYo/src/pool.jl:419 [inlined]\n",
      "  [5] #alloc#990\n",
      "    @ ~/.julia/packages/CUDA/tVtYo/src/pool.jl:409 [inlined]\n",
      "  [6] alloc\n",
      "    @ ~/.julia/packages/CUDA/tVtYo/src/pool.jl:403 [inlined]\n",
      "  [7] CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}(#unused#::UndefInitializer, dims::NTuple{4, Int64})\n",
      "    @ CUDA ~/.julia/packages/CUDA/tVtYo/src/array.jl:93\n",
      "  [8] similar\n",
      "    @ ~/.julia/packages/CUDA/tVtYo/src/array.jl:213 [inlined]\n",
      "  [9] #batchnorm#64\n",
      "    @ ~/.julia/packages/NNlib/aaK3U/ext/NNlibCUDACUDNNExt/batchnorm.jl:37 [inlined]\n",
      " [10] batchnorm\n",
      "    @ ~/.julia/packages/NNlib/aaK3U/ext/NNlibCUDACUDNNExt/batchnorm.jl:35 [inlined]\n",
      " [11] _batchnorm_cudnn!\n",
      "    @ ~/.julia/packages/LuxLib/2CJpI/ext/LuxLibLuxCUDAExt.jl:43 [inlined]\n",
      " [12] rrule\n",
      "    @ ~/.julia/packages/LuxLib/2CJpI/ext/LuxLibLuxCUDAExt.jl:55 [inlined]\n",
      " [13] rrule\n",
      "    @ ~/.julia/packages/ChainRulesCore/0t04l/src/rules.jl:134 [inlined]\n",
      " [14] chain_rrule\n",
      "    @ ~/.julia/packages/Zygote/JeHtr/src/compiler/chainrules.jl:223 [inlined]\n",
      " [15] macro expansion\n",
      "    @ ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:101 [inlined]\n",
      " [16] _pullback\n",
      "    @ ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:101 [inlined]\n",
      " [17] _pullback\n",
      "    @ ~/.julia/packages/LuxLib/2CJpI/ext/LuxLibLuxCUDAExt.jl:30 [inlined]\n",
      " [18] _pullback\n",
      "    @ ~/.julia/packages/LuxLib/2CJpI/ext/LuxLibLuxCUDAExt.jl:20 [inlined]\n",
      " [19] _pullback(::Zygote.Context{false}, ::typeof(Core.kwcall), ::NamedTuple{(:momentum, :epsilon, :training), Tuple{Float32, Float32, Val{true}}}, ::typeof(batchnorm), ::CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, ::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, ::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, ::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, ::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [20] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/normalize.jl:135 [inlined]\n",
      " [21] _pullback(::Zygote.Context{false}, ::BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ::CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, ::NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, ::NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [22] _pullback\n",
      "    @ ~/.julia/packages/LuxCore/yC3wg/src/LuxCore.jl:100 [inlined]\n",
      " [23] macro expansion\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:0 [inlined]\n",
      " [24] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:493 [inlined]\n",
      " [25] _pullback(::Zygote.Context{false}, ::typeof(Lux.applychain), ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, ::CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [26] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:491 [inlined]\n",
      " [27] _pullback\n",
      "    @ ~/.julia/packages/LuxCore/yC3wg/src/LuxCore.jl:100 [inlined]\n",
      " [28] macro expansion\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:0 [inlined]\n",
      " [29] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:146 [inlined]\n",
      " [30] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:144 [inlined]\n",
      " [31] _pullback\n",
      "    @ ~/.julia/packages/LuxCore/yC3wg/src/LuxCore.jl:100 [inlined]\n",
      " [32] macro expansion\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:0 [inlined]\n",
      " [33] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:493 [inlined]\n",
      " [34] _pullback(::Zygote.Context{false}, ::typeof(Lux.applychain), ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, ::CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [35] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:491 [inlined]\n",
      " [36] _pullback\n",
      "    @ ~/.julia/packages/LuxCore/yC3wg/src/LuxCore.jl:100 [inlined]\n",
      " [37] macro expansion\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:0 [inlined]\n",
      " [38] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:146 [inlined]\n",
      " [39] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:144 [inlined]\n",
      " [40] _pullback\n",
      "    @ ~/.julia/packages/LuxCore/yC3wg/src/LuxCore.jl:100 [inlined]\n",
      " [41] macro expansion\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:0 [inlined]\n",
      " [42] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:493 [inlined]\n",
      " [43] _pullback(::Zygote.Context{false}, ::typeof(Lux.applychain), ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, WrappedFunction{var\"#27#29\"}}}, ::CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}}}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [44] _pullback\n",
      "    @ ~/.julia/packages/Lux/5YzHA/src/layers/containers.jl:491 [inlined]\n",
      " [45] _pullback(::Zygote.Context{false}, ::Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{Parallel{NamedTuple{(:layer_1, :layer_2), Tuple{Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, MaxPool{2, 4}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, ConvTranspose{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}, Chain{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}}}, Nothing}}}, Nothing, var\"#34#35\"}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, BatchNorm{true, true, Float32, typeof(leakyrelu), typeof(zeros32), typeof(ones32)}, Conv{2, true, 4, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}, WrappedFunction{var\"#27#29\"}}}, Nothing}, ::CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}}}, ::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:running_mean, :running_var, :training), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Val{true}}}, NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [46] _pullback\n",
      "    @ ~/Desktop/Project BAC/BAC project/4_train_with_fluxMPI.ipynb:5 [inlined]\n",
      " [47] _pullback(ctx::Zygote.Context{false}, f::var\"#lossfn2#37\"{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}, args::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:0\n",
      " [48] pullback(f::Function, cx::Zygote.Context{false}, args::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface.jl:44\n",
      " [49] pullback(f::Function, args::NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7), Tuple{NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19, :layer_20, :layer_21, :layer_22, :layer_23, :layer_24, :layer_25, :layer_26), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14, :layer_15, :layer_16, :layer_17, :layer_18, :layer_19), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9, :layer_10, :layer_11, :layer_12, :layer_13, :layer_14), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4, :layer_5, :layer_6, :layer_7, :layer_8, :layer_9), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2, :layer_3, :layer_4), Tuple{NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:scale, :bias), Tuple{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(), Tuple{}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface.jl:42\n",
      " [50] top-level scope\n",
      "    @ ~/Desktop/Project BAC/BAC project/4_train_with_fluxMPI.ipynb:6"
     ]
    }
   ],
   "source": [
    "for epoch in 1:1\n",
    "\tfor (_x, _y) in train_loader\n",
    "\t\tglobal ps, st_opt\n",
    "\t\tx, y = Float32.(_x) |> gpu, Float32.(_y) |> gpu\n",
    "\t\tlossfn2(p) = dice_loss(model(x, p, st)[1], y)\n",
    "\t\tl, back = Zygote.pullback(lossfn2, ps)\n",
    "\t\tFluxMPI.fluxmpi_println(\"\\tLoss $l\")\n",
    "\t\tgs = back(one(l))[1]\n",
    "\t\tst_opt, ps = Optimisers.update(st_opt, ps, gs)\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
